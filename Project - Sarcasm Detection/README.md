In conclusion, the sarcasm detection task involved a series of preprocessing steps, including handling contractions, converting text to lowercase, lemmatization, and punctuation removal. These processed texts were then embedded using the GloVe model. The experimental phase featured the training of two models: a Bidirectional LSTM model yielded a promising 83% accuracy, while the BERT pre-trained model achieved a reasonable 57% accuracy in sarcasm detection.
